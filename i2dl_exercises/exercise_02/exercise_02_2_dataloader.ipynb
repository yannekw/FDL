{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - DataLoader\n",
    "In the previous notebook you have implemented a dataset that we can now use to access our data. However, in machine learning, we often need to perform a few additional data preparation steps before we can start training models.\n",
    "\n",
    "An important additional class for data preparation is the **DataLoader**. By wrapping a dataset in a dataloader, we will be able to load small subsets of the dataset at a time, instead of having to load each sample separately. In machine learning, the small subsets are referred to as **mini-batches**, which will play an important role later in the lecture.\n",
    "\n",
    "In this notebook, you will implement your own dataloader, which you can then use to load mini-batches from the dataset you implemented previously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Group Information\n",
    "- Group Number: Group 11\n",
    "- Group Members: Malik Abdoul Hamidou, Maurice Todtenbier, Niclas Gambal, Yannek Wunderlich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Mount folder in Colab\n",
    "\n",
    "Uncomment thefollowing cell to mount your gdrive if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T20:34:37.395261Z",
     "start_time": "2025-11-03T20:34:37.387746Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom google.colab import drive\\nimport os\\n\\ngdrive_path='/content/gdrive/MyDrive/i2dl/exercise_03'\\n\\n# This will mount your google drive uWnder 'MyDrive'\\ndrive.mount('/content/gdrive', force_remount=True)\\n# In order to access the files in this notebook we have to navigate to the correct folder\\nos.chdir(gdrive_path)\\n# Check manually if all files are present\\nprint(sorted(os.listdir()))\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the following lines if you want to use Google Colab\n",
    "# We presume you created a folder \"i2dl\" within your main drive folder, and put the exercise there.\n",
    "# NOTE: terminate all other colab sessions that use GPU!\n",
    "# NOTE 2: Make sure the correct exercise folder (e.g exercise_03) is given.\n",
    "# NOTE 3: To use, remove the triple \" from the beginning and end of this cell.\n",
    "\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_03'\n",
    "\n",
    "# This will mount your google drive uWnder 'MyDrive'\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "# In order to access the files in this notebook we have to navigate to the correct folder\n",
    "os.chdir(gdrive_path)\n",
    "# Check manually if all files are present\n",
    "print(sorted(os.listdir()))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "First, you need to import libraries and code, as always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T20:34:40.025197Z",
     "start_time": "2025-11-03T20:34:37.436242Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPython \u001b[36m3.12.12\u001b[39m\n",
      "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
      "\u001b[2mInstalled \u001b[1m38 packages\u001b[0m \u001b[2min 2.46s\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1masttokens\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcolorama\u001b[0m\u001b[2m==0.4.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcomm\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcontourpy\u001b[0m\u001b[2m==1.3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcycler\u001b[0m\u001b[2m==0.12.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdebugpy\u001b[0m\u001b[2m==1.8.17\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdecorator\u001b[0m\u001b[2m==5.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mexecuting\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfonttools\u001b[0m\u001b[2m==4.60.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mipykernel\u001b[0m\u001b[2m==7.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mipynb\u001b[0m\u001b[2m==0.5.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mipython\u001b[0m\u001b[2m==9.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mipython-pygments-lexers\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjedi\u001b[0m\u001b[2m==0.19.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==8.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjupyter-core\u001b[0m\u001b[2m==5.9.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mkiwisolver\u001b[0m\u001b[2m==1.4.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmatplotlib\u001b[0m\u001b[2m==3.10.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmatplotlib-inline\u001b[0m\u001b[2m==0.1.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnest-asyncio\u001b[0m\u001b[2m==1.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.3.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==25.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mparso\u001b[0m\u001b[2m==0.8.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==12.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mplatformdirs\u001b[0m\u001b[2m==4.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprompt-toolkit\u001b[0m\u001b[2m==3.0.52\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==7.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpure-eval\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpygments\u001b[0m\u001b[2m==2.19.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyparsing\u001b[0m\u001b[2m==3.2.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dateutil\u001b[0m\u001b[2m==2.9.0.post0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==27.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msix\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mstack-data\u001b[0m\u001b[2m==0.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtornado\u001b[0m\u001b[2m==6.5.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtraitlets\u001b[0m\u001b[2m==5.14.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwcwidth\u001b[0m\u001b[2m==0.2.14\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv sync --frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T20:34:40.322864Z",
     "start_time": "2025-11-03T20:34:40.106362Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from exercise_code.data import DataLoader, DummyDataset\n",
    "from exercise_code.tests import (\n",
    "    test_dataloader, \n",
    "    test_dataloader_len,\n",
    "    test_dataloader_iter,\n",
    "    save_pickle, \n",
    "    load_pickle\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # To prevent the kernel from dying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Iterating over a Dataset\n",
    "Throughout this notebook a dummy dataset will be used that contains all even numbers from 2 to 100. Similar to the dataset you have implemented before, the dummy dataset has a `__len__()` method that allows us to call `len(dataset)`, as well as a `__getitem__()` method, which allows you to call `dataset[i]` and returns a dict `{\"data\": val}` where `val` is the i-th even number. If you would like to see the code, have a look at `DummyDataset` in `exercise_code/data/base_dataset.py`.\n",
    "\n",
    "Let's start by defining the dataset, and calling its methods to get a better feel for it.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement (1 Point)</h3>\n",
    "    <p>Print the length, first and last element of the dataset.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T20:34:40.368869Z",
     "start_time": "2025-11-03T20:34:40.342391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Length:\t 50 \n",
      "First Element:\t {'data': 2} \n",
      "Last Element:\t {'data': 100}\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.data.base_dataset import DummyDataset\n",
    "\n",
    "dataset = DummyDataset(\n",
    "    root=None,\n",
    "    divisor=2,\n",
    "    limit=100\n",
    ")\n",
    "print(\n",
    "    \"Dataset Length:\\t\", len(dataset), # TODO: Print the length of the dataset\n",
    "    \"\\nFirst Element:\\t\", dataset[0], # TODO: Print the first element of the dataset\n",
    "    \"\\nLast Element:\\t\", dataset[len(dataset) - 1], # TODO: Print the last element of the dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, you will write some code to iterate over the dataset in mini-batches, similarly to what a dataloader is supposed to do. The number of samples to load per mini-batch is called a **batch size**. For the rest of this notebook, the batch size is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T20:34:40.412777Z",
     "start_time": "2025-11-03T20:34:40.387903Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now define a simple function that iterates over the dataset and groups samples into mini-batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T20:34:40.458861Z",
     "start_time": "2025-11-03T20:34:40.426291Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_batches(dataset, batch_size):\n",
    "    batches = []  # list of all mini-batches\n",
    "    batch = []  # current mini-batch\n",
    "    for i in range(len(dataset)):\n",
    "        batch.append(dataset[i])\n",
    "        if len(batch) == batch_size:  # if the current mini-batch is full,\n",
    "            batches.append(batch)  # add it to the list of mini-batches,\n",
    "            batch = []  # and start a new mini-batch\n",
    "    return batches\n",
    "\n",
    "batches = build_batches(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the mini-batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T20:34:40.500089Z",
     "start_time": "2025-11-03T20:34:40.470031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini-batch 0: [{'data': 2}, {'data': 4}, {'data': 6}]\n",
      "mini-batch 1: [{'data': 8}, {'data': 10}, {'data': 12}]\n",
      "mini-batch 2: [{'data': 14}, {'data': 16}, {'data': 18}]\n",
      "mini-batch 3: [{'data': 20}, {'data': 22}, {'data': 24}]\n",
      "mini-batch 4: [{'data': 26}, {'data': 28}, {'data': 30}]\n",
      "mini-batch 5: [{'data': 32}, {'data': 34}, {'data': 36}]\n",
      "mini-batch 6: [{'data': 38}, {'data': 40}, {'data': 42}]\n",
      "mini-batch 7: [{'data': 44}, {'data': 46}, {'data': 48}]\n",
      "mini-batch 8: [{'data': 50}, {'data': 52}, {'data': 54}]\n",
      "mini-batch 9: [{'data': 56}, {'data': 58}, {'data': 60}]\n",
      "mini-batch 10: [{'data': 62}, {'data': 64}, {'data': 66}]\n",
      "mini-batch 11: [{'data': 68}, {'data': 70}, {'data': 72}]\n",
      "mini-batch 12: [{'data': 74}, {'data': 76}, {'data': 78}]\n",
      "mini-batch 13: [{'data': 80}, {'data': 82}, {'data': 84}]\n",
      "mini-batch 14: [{'data': 86}, {'data': 88}, {'data': 90}]\n",
      "mini-batch 15: [{'data': 92}, {'data': 94}, {'data': 96}]\n"
     ]
    }
   ],
   "source": [
    "def print_batches(batches):  \n",
    "    for i, batch in enumerate(batches):\n",
    "        print(\"mini-batch %d:\" % i, str(batch))\n",
    "\n",
    "print_batches(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the iteration works, but the output is not very pretty. Let us now write a simple function that combines the dictionaries of all samples in a mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T20:34:40.541684Z",
     "start_time": "2025-11-03T20:34:40.515120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini-batch 0: {'data': [2, 4, 6]}\n",
      "mini-batch 1: {'data': [8, 10, 12]}\n",
      "mini-batch 2: {'data': [14, 16, 18]}\n",
      "mini-batch 3: {'data': [20, 22, 24]}\n",
      "mini-batch 4: {'data': [26, 28, 30]}\n",
      "mini-batch 5: {'data': [32, 34, 36]}\n",
      "mini-batch 6: {'data': [38, 40, 42]}\n",
      "mini-batch 7: {'data': [44, 46, 48]}\n",
      "mini-batch 8: {'data': [50, 52, 54]}\n",
      "mini-batch 9: {'data': [56, 58, 60]}\n",
      "mini-batch 10: {'data': [62, 64, 66]}\n",
      "mini-batch 11: {'data': [68, 70, 72]}\n",
      "mini-batch 12: {'data': [74, 76, 78]}\n",
      "mini-batch 13: {'data': [80, 82, 84]}\n",
      "mini-batch 14: {'data': [86, 88, 90]}\n",
      "mini-batch 15: {'data': [92, 94, 96]}\n"
     ]
    }
   ],
   "source": [
    "def combine_batch_dicts(batch):\n",
    "    batch_dict = {}\n",
    "    for data_dict in batch:\n",
    "        for key, value in data_dict.items():\n",
    "            if key not in batch_dict:\n",
    "                batch_dict[key] = []\n",
    "            batch_dict[key].append(value)\n",
    "    return batch_dict\n",
    "\n",
    "combined_batches = [combine_batch_dicts(batch) for batch in batches]\n",
    "print_batches(combined_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much more organized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform operations more efficiently later, we would also like the values of the mini-batches to be contained in a numpy array instead of a simple list. Let's briefly write a function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T20:34:40.583256Z",
     "start_time": "2025-11-03T20:34:40.553200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini-batch 0: {'data': array([2, 4, 6])}\n",
      "mini-batch 1: {'data': array([ 8, 10, 12])}\n",
      "mini-batch 2: {'data': array([14, 16, 18])}\n",
      "mini-batch 3: {'data': array([20, 22, 24])}\n",
      "mini-batch 4: {'data': array([26, 28, 30])}\n",
      "mini-batch 5: {'data': array([32, 34, 36])}\n",
      "mini-batch 6: {'data': array([38, 40, 42])}\n",
      "mini-batch 7: {'data': array([44, 46, 48])}\n",
      "mini-batch 8: {'data': array([50, 52, 54])}\n",
      "mini-batch 9: {'data': array([56, 58, 60])}\n",
      "mini-batch 10: {'data': array([62, 64, 66])}\n",
      "mini-batch 11: {'data': array([68, 70, 72])}\n",
      "mini-batch 12: {'data': array([74, 76, 78])}\n",
      "mini-batch 13: {'data': array([80, 82, 84])}\n",
      "mini-batch 14: {'data': array([86, 88, 90])}\n",
      "mini-batch 15: {'data': array([92, 94, 96])}\n"
     ]
    }
   ],
   "source": [
    "def batch_to_numpy(batch):\n",
    "    numpy_batch = {}\n",
    "    for key, value in batch.items():\n",
    "        numpy_batch[key] = np.array(value)\n",
    "    return numpy_batch\n",
    "\n",
    "numpy_batches = [batch_to_numpy(batch) for batch in combined_batches]\n",
    "print_batches(numpy_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we would like to make the loading a bit more memory efficient. Instead of loading the entire dataset into memory at once, let us only load samples when they are needed. This can also be done by building a Python generator, using the `yield` keyword.\n",
    "\n",
    "Here are a few tutorials that might be helpful for learning about Python generators and iterators:\n",
    "\n",
    "1. [“Python Iterators and Generators Tutorial”](https://www.datacamp.com/tutorial/python-iterators-generators-tutorial) by DataCamp\n",
    "2. [“How to Use Generators and yield in Python”](https://realpython.com/introduction-to-python-generators/) by Real Python \n",
    "3. [“Python Generators (With Examples)”](https://www.programiz.com/python-programming/generator) by Programiz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T20:34:40.629245Z",
     "start_time": "2025-11-03T20:34:40.598786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini-batch 0: {'data': array([2, 4, 6])}\n",
      "mini-batch 1: {'data': array([ 8, 10, 12])}\n",
      "mini-batch 2: {'data': array([14, 16, 18])}\n",
      "mini-batch 3: {'data': array([20, 22, 24])}\n",
      "mini-batch 4: {'data': array([26, 28, 30])}\n",
      "mini-batch 5: {'data': array([32, 34, 36])}\n",
      "mini-batch 6: {'data': array([38, 40, 42])}\n",
      "mini-batch 7: {'data': array([44, 46, 48])}\n",
      "mini-batch 8: {'data': array([50, 52, 54])}\n",
      "mini-batch 9: {'data': array([56, 58, 60])}\n",
      "mini-batch 10: {'data': array([62, 64, 66])}\n",
      "mini-batch 11: {'data': array([68, 70, 72])}\n",
      "mini-batch 12: {'data': array([74, 76, 78])}\n",
      "mini-batch 13: {'data': array([80, 82, 84])}\n",
      "mini-batch 14: {'data': array([86, 88, 90])}\n",
      "mini-batch 15: {'data': array([92, 94, 96])}\n"
     ]
    }
   ],
   "source": [
    "def build_batch_iterator(dataset, batch_size, shuffle=True):\n",
    "    if shuffle:\n",
    "        index_iterator = iter(np.random.permutation(len(dataset)))  # define indices as iterator\n",
    "    else:\n",
    "        index_iterator = iter(range(len(dataset)))  # define indices as iterator\n",
    "\n",
    "    batch = []\n",
    "    for index in index_iterator:  # iterate over indices using the iterator\n",
    "        batch.append(dataset[index])\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch  # use yield keyword to define a iterable generator\n",
    "            batch = []\n",
    "            \n",
    "batch_iterator = build_batch_iterator(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "batches = []\n",
    "for batch in batch_iterator:\n",
    "    batches.append(batch)\n",
    "\n",
    "print_batches(\n",
    "    [batch_to_numpy(combine_batch_dicts(batch)) for batch in batches]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functionality of the cell above is now pretty close to what the dataloader is supposed to do. However, there are still two remaining issues:\n",
    "1. The last two samples of the dataset are not contained in any mini-batch. This is because the number of samples in the dataset is not dividable by the batch size, so there are a few left-over samples which are implicitly discarded. Ideally, an option would be prefered that allows you to decide how to handle these last samples.\n",
    "2. The order of the mini-batches, as well as the fact which samples are grouped together, is always in increasing order. Ideally, there should be another option that allows you to randomize which samples are grouped together. The randomization could be easily implemented by randomly permuting the indices of the dataset before iterating over it, e.g. using `indices = np.random.permutation(len(dataset))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DataLoader Class Implementation\n",
    "Now it is your turn to put everything together and implement the DataLoader as a proper class.\n",
    "We provide you with a basic skeleton for this, which you can find in `class DataLoader` of `exercise_code/data/dataloader.py`. Open the file and have a look at the class. Note that the `__init__` method receives four arguments:\n",
    "* **dataset** is the dataset that the dataloader should load.\n",
    "* **batch_size** is the mini-batch size, i.e. the number of samples you want to load at a time.\n",
    "* **shuffle** is a binary boolean (True or False) and defines whether the dataset should be randomly shuffled or not.\n",
    "* **drop_last**: is binary and defines how to handle the last mini-batch in your dataset. Specifically, if the amount of samples in your dataset is not dividable by the mini-batch size, there will be some samples left over in the end. If `drop_last=True`, we simply discard those samples, otherwise we return them together as a smaller mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement (1 Points)</h3>\n",
    "    <p>Implement the <code>__len__(self)</code> method in <code>exercise_code/data/dataloader.py</code>. </p>\n",
    "    <p><b>Hint:</b> Don't forget to think about drop_last! We will test for both modes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T20:34:40.671436Z",
     "start_time": "2025-11-03T20:34:40.641262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####### Testing Started #######\n",
      "\n",
      "\u001b[93m\u001b[4mTesting __len__()\u001b[0m with condition: \u001b[94mdrop_last=True\u001b[0m. No. of test cases: 2\n",
      "Test LenTestInt: \u001b[92mpassed!\u001b[0m\n",
      "Test LenTestCorrect: \u001b[92mpassed!\u001b[0m\n",
      "Method __len__() (using drop_last=True): \u001b[92mpassed!\u001b[0m -->  Tests passed: \u001b[92m2\u001b[0m/\u001b[92m2\u001b[0m\n",
      "\n",
      "\u001b[93m\u001b[4mTesting __len__()\u001b[0m with condition: \u001b[94mdrop_last=False\u001b[0m. No. of test cases: 2\n",
      "Test LenTestInt: \u001b[92mpassed!\u001b[0m\n",
      "Test LenTestCorrect: \u001b[92mpassed!\u001b[0m\n",
      "Method __len__() (using drop_last=False): \u001b[92mpassed!\u001b[0m -->  Tests passed: \u001b[92m2\u001b[0m/\u001b[92m2\u001b[0m\n",
      "\n",
      "\u001b[93m\u001b[4mTesting __len__()\u001b[0m with condition: \u001b[94mdrop_last=False; batch_size=1\u001b[0m. No. of test cases: 2\n",
      "Test LenTestInt: \u001b[92mpassed!\u001b[0m\n",
      "Test LenTestCorrect: \u001b[92mpassed!\u001b[0m\n",
      "Method __len__() (using drop_last=False; batch_size=1): \u001b[92mpassed!\u001b[0m -->  Tests passed: \u001b[92m2\u001b[0m/\u001b[92m2\u001b[0m\n",
      "\n",
      "\n",
      "####### Testing Finished #######\n",
      "Method __len__(): \u001b[92mpassed!\u001b[0m -->  Tests passed: \u001b[92m6\u001b[0m/\u001b[92m6\u001b[0m\n",
      "Score: \u001b[92m100\u001b[0m/\u001b[92m100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.data.dataloader import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "_ = test_dataloader_len(\n",
    "    dataloader=dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement (3 Point)</h3>\n",
    "    <p>Implement the <code>__iter__(self)</code> method in <code>exercise_code/data/dataloader.py</code>. </p>\n",
    "    <p><b>Hint:</b> Make use of the code in '1. Iterating over a Dataset' when implementing your <code>__iter__()</code> method. We are again testing for both drop_last modes! \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T20:34:40.722076Z",
     "start_time": "2025-11-03T20:34:40.693483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####### Testing Started #######\n",
      "\n",
      "\u001b[93m\u001b[4mTesting __iter__()\u001b[0m with condition: \u001b[94mdrop_last=True\u001b[0m. No. of test cases: 8\n",
      "Test IterTestIterable: \u001b[92mpassed!\u001b[0m\n",
      "Test IterTestItemType: \u001b[92mpassed!\u001b[0m\n",
      "Test IterTestBatchSize: \u001b[92mpassed!\u001b[0m\n",
      "Test IterTestNumBatches: \u001b[92mpassed!\u001b[0m\n",
      "Test IterTestValuesUnique: \u001b[92mpassed!\u001b[0m\n",
      "Test IterTestValueRange: \u001b[92mpassed!\u001b[0m\n",
      "Test IterTestShuffled: \u001b[92mpassed!\u001b[0m\n",
      "Test IterTestNonDeterministic: \u001b[92mpassed!\u001b[0m\n",
      "Method __iter__() (using drop_last=True): \u001b[92mpassed!\u001b[0m -->  Tests passed: \u001b[92m8\u001b[0m/\u001b[92m8\u001b[0m\n",
      "\n",
      "\u001b[93m\u001b[4mTesting __iter__()\u001b[0m with condition: \u001b[94mdrop_last=False\u001b[0m. No. of test cases: 8\n",
      "Test IterTestIterable: \u001b[92mpassed!\u001b[0m\n",
      "Test IterTestItemType: \u001b[92mpassed!\u001b[0m\n",
      "Test IterTestBatchSize: \u001b[92mpassed!\u001b[0m\n",
      "Test IterTestNumBatches: \u001b[92mpassed!\u001b[0m\n",
      "Test IterTestValuesUnique: \u001b[92mpassed!\u001b[0m\n",
      "Test IterTestValueRange: \u001b[92mpassed!\u001b[0m\n",
      "Test IterTestShuffled: \u001b[92mpassed!\u001b[0m\n",
      "Test IterTestNonDeterministic: \u001b[92mpassed!\u001b[0m\n",
      "Method __iter__() (using drop_last=False): \u001b[92mpassed!\u001b[0m -->  Tests passed: \u001b[92m8\u001b[0m/\u001b[92m8\u001b[0m\n",
      "\n",
      "\n",
      "####### Testing Finished #######\n",
      "Method __iter__(): \u001b[92mpassed!\u001b[0m -->  Tests passed: \u001b[92m16\u001b[0m/\u001b[92m16\u001b[0m\n",
      "Score: \u001b[92m100\u001b[0m/\u001b[92m100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.data.dataloader import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "_ = test_dataloader_iter(\n",
    "    dataloader=dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're done, run the cells below to check if your dataloader works as intended. You can change the value of drop_last to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T20:34:40.774663Z",
     "start_time": "2025-11-03T20:34:40.747112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([ 4, 94, 90])}\n",
      "{'data': array([84,  8, 48])}\n",
      "{'data': array([38, 28,  6])}\n",
      "{'data': array([76, 36, 30])}\n",
      "{'data': array([12, 68, 60])}\n",
      "{'data': array([20, 74, 56])}\n",
      "{'data': array([34, 70, 64])}\n",
      "{'data': array([100,  46,  86])}\n",
      "{'data': array([32, 58, 52])}\n",
      "{'data': array([22, 18,  2])}\n",
      "{'data': array([62, 42, 16])}\n",
      "{'data': array([72, 50, 82])}\n",
      "{'data': array([26, 66, 98])}\n",
      "{'data': array([80, 96, 14])}\n",
      "{'data': array([78, 88, 40])}\n",
      "{'data': array([10, 24, 92])}\n",
      "{'data': array([44, 54])}\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.data.dataloader import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,    # Change here if you want to see the impact of drop last and check out the last batch\n",
    ")\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your DataLoaders for Submission\n",
    "Simply save your dataloaders using the following cell. This will save them as well as dataset from the first notebook to a pickle file `cifar_dataset_and_loader.p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T20:34:40.849969Z",
     "start_time": "2025-11-03T20:34:40.809938Z"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.data.dataloader import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "dataset = load_pickle(\"cifar_dataset.p\") # load dataset from the pickle file saved in notebook 1\n",
    "\n",
    "save_pickle(\n",
    "    data_dict={\n",
    "        \"dataset\": dataset['dataset'],\n",
    "        \"cifar_mean\": dataset['cifar_mean'],\n",
    "        \"cifar_std\": dataset['cifar_std'],\n",
    "        \"dataloader\": dataloader\n",
    "    },\n",
    "    file_name=\"cifar_dataset_and_loader.p\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h3>Note</h3>\n",
    "    <p>Note that <b>this is the ONLY file you need to submit</b>. Each time you make changes in either <code>dataset</code> or <code>dataloaders</code>, you need to <b>rerun the following code</b> to save your changes for submission.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Instructions\n",
    "\n",
    "Now, that you have completed the necessary parts in the notebook, you can go on and submit your files.\n",
    "\n",
    "Go to the OpenOlat course:\n",
    "https://lms.uni-kiel.de/url/RepositoryEntry/5692096707\n",
    "\n",
    "Go under Exercises > Exercise 01\n",
    "\n",
    "!!! You only see this if you are enrolled into one of the groups !!!\n",
    "\n",
    "Submit your exercise there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T20:34:40.942151Z",
     "start_time": "2025-11-03T20:34:40.871493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant folders: ['exercise_code', 'models']\n",
      "notebooks files: ['exercise_02_1_cifar10.ipynb', 'exercise_02_2_dataloader.ipynb']\n",
      "Adding folder exercise_code\n",
      "Adding folder models\n",
      "Adding notebook exercise_02_1_cifar10.ipynb\n",
      "Adding notebook exercise_02_2_dataloader.ipynb\n",
      "Zipping successful! Zip is stored under: C:\\Users\\Yanne\\Desktop\\Foundations of deep learning\\GIT\\i2dl_exercises\\output\\exercise_02_group_11.zip\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.submit import submit_exercise\n",
    "\n",
    "submit_exercise('../output/exercise_02_group_11.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Goals\n",
    "\n",
    "For this exercise we only test your implementations which are tested throughout both notebooks. Here is a list of test cases that will be evaluated on the server using your `ImageFolderDataset` as well as `DataLoader` classes. Here is an overview split among our two notebooks:\n",
    "\n",
    "- Goal for **notebook 1**: Implement an ImageFolderDataset with transforms for rescaling and normalizing.\n",
    "    - To implement: \n",
    "        1. `exercise_code/data/image_folder_dataset.py`: `ImageFolderDataset` -  `__len__()`, `__getitem()__`\n",
    "        2. `exercise_code/data/image_folder_dataset.py`: `RescaleTransform`\n",
    "        3. `exercise_code/data/image_folder_dataset.py`: `compute_image_mean_and_std()`\n",
    "    - Test cases:\n",
    "      1. Does `__len__()` of `ImageFolderDataset` return the correct data type?\n",
    "      2. Does `__len__()` of `ImageFolderDataset` return the correct value?\n",
    "      3. Does `__getitem()__` of `ImageFolderDataset` return the correct data type?\n",
    "      4. Does `__getitem()__` of `ImageFolderDataset` load images as numpy arrays with correct shape?\n",
    "      5. Do values after rescaling with `RescaleTransform` have the correct minimum?\n",
    "      6. Do values after rescaling with `RescaleTransform` have the correct maximum?\n",
    "      7. Does `compute_image_mean_and_std()` compute the correct mean?\n",
    "      8. Does `compute_image_mean_and_std()` compute the correct std?\n",
    "\n",
    "\n",
    "- Goal for **notebook 2**: Implement a DataLoader that loads mini-batches from a given dataset and supports batch_size, shuffle, and drop_last args.\n",
    "    - Test cases:\n",
    "      1. Does `__len__()` return the correct data type?\n",
    "      2. Does `__len__()` return the correct value?\n",
    "      3. Does `__iter__()` work at all, i.e. is it possible to iterate over the dataloader?\n",
    "      4. Does `__iter__()` load the correct data type?\n",
    "      5. Does `__iter__()` load data with correct batch size?\n",
    "      6. Does `__iter__()` load the correct number of batches?\n",
    "      7. Does `__iter__()` load every sample only once?\n",
    "      8. Does `__iter__()` load the smallest and largest sample from the dataset?\n",
    "      9. Does `__iter__()` shuffle the data correctly (if necessary)?\n",
    "      10. Does `__iter__()` return non-deterministic values when shuffling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Takeaways\n",
    "1. In machine learning, we often need to load data in **mini-batches**, which are small subsets of the training dataset. How many samples to load per mini-batch is called the **batch size**.\n",
    "2. In addition to the Dataset class, we use a **DataLoader** class that takes care of mini-batch construction, data shuffling, and more.\n",
    "3. The dataloader is iterable and only loads those samples of the dataset that are needed for the current mini-batch. This can lead to bottlenecks later if you are unable to provide enough batches in time for your upcoming pipeline. This is especially true when loading from HDDs as the slow reading time can be a bottleneck in your complete pipeline later.\n",
    "4. The dataloader task can easily by distributed amongst multiple processes as well as pre-fetched. When we switch to PyTorch later we can directly use our dataset classes and replace our current Dataloader with theirs :)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9fd0696167aaed30d55c31fd713b4c23f5cf987c5457682b34d44d5309ecf99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
